import json
from langchain.prompts import PromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List, Optional
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# set output parser, i.e. expected output format.
class RQMultiple(BaseModel):
    reformulated_queries: List[str] = Field(description="list of reformulated queries generated by LLM.")
class CQMultiple(BaseModel):
    clarification_questions: List[str] = Field(description="list of clarification questions generated by LLM.")

def parse(output,parser):
    output = output[0]["generated_text"].split("assistant<|end_header_id|>")[-1].strip("\n").strip() # keep only the assistant message
    try: 
        start_ix, end_ix = output.index('{'), output.index('}')
        parsed_output = parser.parse(output[start_ix:end_ix]).dict()
        return True, parsed_output
    except:
        return False, {}
    
# system instructions
rq_system_instruction = """Given a query in an information-seeking system, generate 5 reformulated queries that clarify the original query to gain a better understanding of the user's intention. Imagine that the user will select the reformulated query that most accurately describes their intention in each turn."""
cq_systen_instruction = """Given a query in an information-seeking system, generate a clarification question to gain a better understanding of the user's intention."""

# few-shot examples
fs_examples = json.load(open("./few_shot_examples.json"))
rq_examples = fs_examples["en"]["select"]["standard"]
cq_examples = fs_examples["en"]["respond"]["standard"]

rq_parser = PydanticOutputParser(pydantic_object=RQMultiple)
cq_parser = PydanticOutputParser(pydantic_object=CQMultiple)

model_id = "meta-llama/Meta-Llama-3-8B-Instruct" # we use an open-source Llama-3 model with instruction tuning
llm_kwargs = {
    "do_sample":True,
    "top_k":10,
    "temperature":0.7
} # hyperparameters for LLM decoding, which will be explained in part 3.

# step 1: tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id,padding_side="left")
tokenizer.pad_token_id = tokenizer.eos_token_id

# step 2: set quantization format, the objective is to change the precision of LLM parameter values to lower
# bit-width representations, therefore reducing memory requirements and enable faster inference.
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True)

# step 3: load LLM and set hyperparameters
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True, # allow loading custom architectures like Llama
)

terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|eot_id|>")] # set end of generation token

llm = transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        eos_token_id=terminators,
        **llm_kwargs
)


GENERATION_TEMPLATE = tokenizer.apply_chat_template(
    [{"role":"system","content":"{system_instruction}\n{format_instruction}"},
     {"role":"user","content":"Here are some examples.\n\n{few_shot_examples}\n"+ \
      "Do not repeat the examples exactly; instead, you should generalize beyond the given examples. Now, focus on the following query.\n\n Query: {query}\n"}],
    add_generation_prompt=True,
    tokenize=False
)

input_query = "obama family tree" # what you want to search 

# build a prompt template that can be used to load different queries
rq_prompt_template = PromptTemplate(template=GENERATION_TEMPLATE,
                        input_variables=["query"],
                        partial_variables={"system_instruction":rq_system_instruction,
                                           "format_instruction":rq_parser.get_format_instructions(),
                                           "few_shot_examples":rq_examples})

# load your query, now it is ready for inference
rq_prompt = rq_prompt_template.format_prompt(query=input_query).text
    
res = llm(rq_prompt)
parsed, res = parse(res,rq_parser)

if parsed:
    rqs = res["reformulated_queries"]
    with open("output.txt",'w') as f:
        f.write(f'Query:{input_query}\n\nReformulated queries:\n'+'\n'.join([f"{i+1}: {rq}" for i, rq in enumerate(rqs)]))
else:
    with open("output.txt",'w') as f:
        f.write("parsing error.")